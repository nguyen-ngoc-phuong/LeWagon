{"cells":[{"cell_type":"markdown","id":"42f4a0d6","metadata":{"lang":"en","id":"42f4a0d6"},"source":["# Classification of trolls on Shapr"]},{"cell_type":"markdown","id":"5bc60d1e","metadata":{"lang":"en","id":"5bc60d1e"},"source":["### The \"Why\" of exercise\n","\n","On social networks, it is important to be able to identify individuals with harmful behavior towards the community. The **moderation problem is a classification**: it is a question of predicting, for a given account, whether it complies with the rules of the community or not.\n","\n","In particular **Shapr**, a professional networking network, realized that a low number of users registered on its platform but exhibited a set of behaviors that were inappropriate and at odds with the purpose of the platform. ."]},{"cell_type":"markdown","id":"055e3662","metadata":{"lang":"en","id":"055e3662"},"source":["### The \"What\"\n","\n","In this exercise, we will use real data from Shapr. In particular we provide:\n","\n","- the result of a manual validation describing whether the account is problematic or not (this is the \"variable to explain\")\n","\n","- as well as a corpus of data describing the new accounts (how long is the text typed in the bio, what other social networks are entered, what is the name of the email provider, etc.): these are the explanatory variables"]},{"cell_type":"markdown","id":"d2f0cf82","metadata":{"lang":"en","id":"d2f0cf82"},"source":["### The \"How\"\n","\n","The practical work reproduces the classification process that was developed internally, and takes place in 2 stages:\n","\n","I. Exploratory Analysis\n","\n","  The exploratory analysis aims to understand \"who\" the problematic profiles are. This is done by studying the ability of explanatory variables to discriminate.\n","\n","II. Modeling and Interpretation of the model\n","\n","  Modeling is done with a classification model, which predicts whether a given profile is problematic based on the explanatory variables. By examining how the model works, we understand its mode of operation, and in particular its use of explanatory variables.\n","\n","\n","Up to you ! And remember: don't get stuck, search, ask your questions, have fun! We are all here to learn and practice."]},{"cell_type":"markdown","id":"8072cf31","metadata":{"lang":"en","id":"8072cf31"},"source":["## 0. [REQUIRED] Creating a new environment"]},{"cell_type":"markdown","id":"cbd3c8dc","metadata":{"lang":"en","id":"cbd3c8dc"},"source":["We will use the [Pycaret](https://pycaret.gitbook.io/docs/) library during this lab.\n","\n","This library requires the creation of a new environment because it uses certain specific versions of certain libraries.\n","\n","By [following this link](https://pycaret.gitbook.io/docs/), create a new environment and install Pycaret.\n","\n","Activate it for the rest of this lab."]},{"cell_type":"markdown","id":"d1488293","metadata":{"lang":"en","id":"d1488293"},"source":["If for some specific reason an error has occurred, don't get stuck and use [Google Colaboratory](https://colab.research.google.com/?utm_source=scs-index) to complete this lab."]},{"cell_type":"markdown","id":"d93cac89","metadata":{"lang":"en","id":"d93cac89"},"source":["## I. Exploratory Analysis"]},{"cell_type":"markdown","id":"c5e0e67d","metadata":{"lang":"en","id":"c5e0e67d"},"source":["### I.a. Loading the dataset"]},{"cell_type":"markdown","id":"2c115587","metadata":{"lang":"en","id":"2c115587"},"source":["Download the `moderation.csv` file at this [address](https://drive.google.com/file/d/1w8geUl5qQ1GonuH7G9YB1h_RrRl4HMgl/view?usp=sharing). It contains different columns describing Shapr profiles:\n","\n","`node_id` ⇨ internal and unique id of each user\n","\n","`email` ⇨ the email used to register (other refers to emails other than the main ones)\n","\n","`has_picture_cover` ⇨ has the user added a photo to their profile?\n","\n","`has_linkedin` ⇨ has the user added their LinkedIn handle?\n","\n","`has_personal_url` ⇨ has the user added a link to their personal url?\n","\n","`has_instagram` ⇨ has the user added their Instagram handle?\n","\n","`tags` ⇨ the set of profile tags, separated by `;`\n","\n","`goals` ⇨ the set of goals chosen by the user, separated by `;`\n","\n","`nb_chars_in_bio` ⇨ the number of characters in their profile bio\n","\n","`is_unwanted` ⇨ has the user been classified as unwanted?\n","\n","The purpose of the lab is to study the variable `is_unwanted` to differentiate a normal account (`is_unwanted == 0`) from a troll (`is_unwanted == 1`)"]},{"cell_type":"markdown","id":"a4dbe71b","metadata":{"lang":"en","id":"a4dbe71b"},"source":["<b>I.1.a)</b> Load the csv file and store it as a Data Frame which you will call `df`. Do you find all the columns described above?"]},{"cell_type":"markdown","id":"adc30732","metadata":{"lang":"en","id":"adc30732"},"source":["<b>I.1.b)</b> Some profiles that have no bio have `nb_chars_in_bio` set to `NaN` (\"Not a Number\"). How many are there? Recode these values ​​to 0 in your dataframe.\n","\n","Do the same by recoding the `tags` and `goals` fields which equal `NaN`, with an empty character string (`\"\"`)."]},{"cell_type":"markdown","id":"c3a31577","metadata":{"lang":"en","id":"c3a31577"},"source":["<b>I.1.c)</b> How many profiles are there in this dataset? How many unwanted profiles?"]},{"cell_type":"markdown","id":"6ac6d067","metadata":{"lang":"en","id":"6ac6d067"},"source":["<b>I.1.d)</b> Check that the `node_id` column contains different values. Does this column seem relevant to you to keep in your classification model?"]},{"cell_type":"markdown","id":"3ce3fd43","metadata":{"lang":"en","id":"3ce3fd43"},"source":["### I.2 Study of explanatory variables\n","\n","We will now look at the explanatory variables, starting with the email providers.\n","\n","<b>I.2.a)</b> How many email providers are there? Represent their distribution in the form of a histogram.\n","\n","<details>\n","<summary><i>Click for a hint</i></summary>\n","    ⟿ Plotly Express has the \"histogram\" function which allows you to make this distribution graph: https://plotly.com/python/histograms/\n","</details>"]},{"cell_type":"markdown","id":"d6b8bbdc","metadata":{"lang":"en","id":"d6b8bbdc"},"source":["<b>I.2.b)</b> We now want to study the link between <b>fournisseur d'emails</b> and <b>probabilité</b> to be labeled as `unwanted`. Calculate this probability according to the email and display it on a graph.\n","\n","\n","<details>\n","<summary><i>Click for a hint</i></summary>\n","    ⟿ Think of `groupby` followed by a `mean` to calculate the probability of `is_unwanted` by different email provider\n","\n","    <br/>\n","\n","    ⟿ The display can use the Plotly Express `bar` function: https://plotly.com/python/bar-charts/\n","</details>"]},{"cell_type":"markdown","id":"c677fbb4","metadata":{"lang":"en","id":"c677fbb4"},"source":["<b>I.2.c)</b> Which email provider is linked to a higher likelihood of being a spam account? What about the lowest probability? Do you think that the email provider is a good explanatory variable?"]},{"cell_type":"markdown","id":"d1129fa2","metadata":{"lang":"en","id":"d1129fa2"},"source":["<b>I.2.d)</b> We now want to link the fact of having given an avatar image and/or informing about its presence on other social networks, with the probability of being an undesirable account.\n","\n","Examine the following code and run it."]},{"cell_type":"code","execution_count":null,"id":"3fbd7686","metadata":{"lang":"en","id":"3fbd7686"},"outputs":[],"source":["col_names = [\"has_picture_cover\", \"has_linkedin\", \"has_twitter\", \"has_personal_url\", \"has_instagram\"]\n","list_probas = []\n","\n","for v in col_names:\n","    proba = df.groupby(v)['is_unwanted'].mean() * 100\n","    print(f\"probability of unwanted count for {v} being 0 or 1: {proba.values}\")\n","    list_probas.append(proba)\n","\n","table_probas = pd.concat(list_probas, axis=1).transpose()\n","table_probas.index = col<_>names\n","\n","px.bar(table_probas, barmode='group')"]},{"cell_type":"markdown","id":"da39a322","metadata":{"lang":"en","id":"da39a322"},"source":["<b>I.2.e)</b> Looking at the graph produced, what do you think of the link between the explanatory variables \"has_picture_cover\", \"has_linkedin\", \"has_twitter\", \"has_personal_url\", \"has_instagram\" and the variable to predict?\n","\n","Does this match your intuition?"]},{"cell_type":"markdown","id":"adc6a785","metadata":{"lang":"en","id":"adc6a785"},"source":["<b>I.2.f)</b> We now want to look at the link between the size of the text entered in the \"bio\", and the fact of being an unwanted account.\n","\n","Show boxplots showing the distribution of character counts in bios (`nb_chars_in_bio`) for normal and junk profiles.\n","\n","\n","<details>\n","<summary><i>Click for a hint</i></summary>\n","    ⟿ The display can use the \"box\" function of Plotly Express: https://plotly.com/python/bar-plots/\n","</details>"]},{"cell_type":"markdown","id":"d15a01d3","metadata":{"lang":"en","id":"d15a01d3"},"source":["<b>I.2.g)</b> Looking at the graph produced, what do you think of the link between the explanatory variable \"nb_chars_in_bio\" and the variable to be predicted?\n","\n","Does this match your intuition?"]},{"cell_type":"markdown","id":"979dc04f","metadata":{"lang":"en","id":"979dc04f"},"source":["<b>I.2.h)</b> We now want to see if `tags` and `goals` have been entered, assuming that a \"troll\" account does not bother to enter them.\n","\n","Create two new columns, `has_goal` and `has_tag`, which are worth `1` when the `goals` and `tags` columns actually contain information.\n","\n","Using the code from question `I.2.d)`, conclude on the relevance of these two variables."]},{"cell_type":"markdown","id":"67fa2c94","metadata":{"lang":"en","id":"67fa2c94"},"source":["## II. Modelization"]},{"cell_type":"markdown","id":"080c2ed4","metadata":{"lang":"en","id":"080c2ed4"},"source":["At the end of the first part, we identified some variables with a clear link to being a spam account:\n","\n","- the size of the bio\n","- the presence or absence of a profile picture\n","- the presence or absence of links to other social networks (LinkedIn, Instagram, Twitter or personal site)\n","- the fact of having entered `tags` and `goals` in the profile"]},{"cell_type":"markdown","id":"1a4a64bf","metadata":{"lang":"en","id":"1a4a64bf"},"source":["### II.1. Data preparation"]},{"cell_type":"markdown","id":"4f73cecd","metadata":{"lang":"en","id":"4f73cecd"},"source":["<b>II.1.a)</b> Construct a DataFrame containing the explanatory variables stated above as well as the variable to be predicted `is_unwanted`.\n","\n","Save this dataframe in a variable named `dataset`. It should contain 10,000 rows and 9 columns."]},{"cell_type":"markdown","id":"9641f829","metadata":{"lang":"en","id":"9641f829"},"source":["<b>II.1.b)</b> As we saw previously, the dataset contains 9 times more normal users (`is_unwanted` being 0) than trolls (`is_unwanted` being 1).\n","\n","Learning a model with this bias is not desirable, because regardless of the user to classify, it would suffice to answer \"no-troll\" to be right in 90% of cases!\n","\n","<b>To guard against this, you are asked to implement a simple strategy of re-balancing the `dataset_train` by \"under-sampling\". This consists of the random retention of a subsample of normal users.</b>\n","\n","Once the rebalancing is complete, check that the `dataset` variable contains 1,000 rows with `is_unwanted` at 0 and 1,000 rows with `is_unwanted` at 1."]},{"cell_type":"markdown","id":"e2344578","metadata":{"lang":"en","id":"e2344578"},"source":["<b>II.1.c)</b> Perform the 80% / 20% split of your dataset with the code below.\n","\n","What is the `stratify` parameter? Why can it be useful here?"]},{"cell_type":"code","execution_count":null,"id":"1719291f-037c-400f-8d23-457b908c6006","metadata":{"id":"1719291f-037c-400f-8d23-457b908c6006"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","dataset_train, dataset_unseen = train_test_split(dataset,\n","                                                 test_size=0.2,\n","                                                 random_state=42,\n","                                                 stratify=dataset['is_unwanted'])"]},{"cell_type":"markdown","id":"38be6ff2","metadata":{"lang":"en","id":"38be6ff2"},"source":["The `stratify=dataset['is_unwanted']` parameter is important here because it ensures that the proportion of unwanted people remains the same after slicing between the train and the test. This is an important point, because an unlucky random slicing could result in a zero-example training dataset of one of the classes, making training impossible."]},{"cell_type":"markdown","id":"1c5e06df","metadata":{"lang":"en","id":"1c5e06df"},"source":["## II.2. Screen models with PyCaret"]},{"cell_type":"markdown","id":"76f10e33","metadata":{"lang":"en","id":"76f10e33"},"source":["<b>II.2.a)</b> Now it's time to prepare the classification with the call to the `setup` function of <b>PyCaret</b>.\n","\n","Complete the following code, justifying your choice for the value of `normalize`."]},{"cell_type":"code","execution_count":null,"id":"d0427f21-3f14-475f-a5f2-080afe54eaa1","metadata":{"id":"d0427f21-3f14-475f-a5f2-080afe54eaa1"},"outputs":[],"source":["from pycaret.classification import setup\n","\n","xp = setup(data = ~~~~ , # to be completed\n","           test_data = ~~~~ , # to be completed\n","           target = ~~~~ , # to be completed\n","           normalize = ~~~~ , # to be completed\n","           session_id = 42,\n","           silent = True,\n","          )"]},{"cell_type":"markdown","id":"fedf7839","metadata":{"lang":"en","id":"fedf7839"},"source":["<b>II.2.a)</b> Sift through PyCaret's models and compare their performance.\n","\n","Which model has the best accuracy?\n","\n","NB: You can specify `exclude=['xgboost', 'catboost']` when sifting to make the calculations much faster. These two algorithms are indeed the most greedy in terms of resources, and (spoiler alert) they do not deliver excellent performance on this problem (/spoiler alert)."]},{"cell_type":"markdown","id":"beecb4c9","metadata":{"lang":"en","id":"beecb4c9"},"source":["<b>II.2.a)</b> Display confusion matrix on test data.\n","\n","What can you say?"]},{"cell_type":"markdown","id":"9dcb7df9","metadata":{"lang":"en","id":"9dcb7df9"},"source":["<b>II.2.b)</b> Based on the numbers in this matrix, calculate the precision and recall in the test sample.\n","\n","Do you observe the same performance as in k-fold validation?"]},{"cell_type":"markdown","id":"29d545cd","metadata":{"lang":"en","id":"29d545cd"},"source":["### III. Bonus"]},{"cell_type":"markdown","id":"ffb7b71d","metadata":{"lang":"en","id":"ffb7b71d"},"source":["<b>III.A)</b> To improve performance, it is possible to vary the model's hyper-parameters.\n","\n","This can be done with a simple call to the `tune_model` function, [described here](https://pycaret.org/tune-model/).\n","\n","Take the best model and optimize its hyper-parameters. Do you see an improvement in performance?"]},{"cell_type":"markdown","id":"1b06423f","metadata":{"lang":"en","id":"1b06423f"},"source":["<b>III.B)</b> Another avenue for improvement lies in the combination of several models, and the <i>blending</i> of their predictions via a majority vote.\n","\n","This can be done with a simple call to the `blend_models` function, [described here](https://pycaret.org/blend_models/).\n","\n","Take the 5 best models and perform an <i>blending</i>. Do you see an improvement in performance in testing?"]},{"cell_type":"markdown","id":"a5889cae","metadata":{"lang":"en","id":"a5889cae"},"source":["<b>III.C)</b> To conclude this lab, we propose to provide an explanation <i>a posteriori</i> of the functioning of the classifier.\n","\n","To obtain the plot of importance of the explanatory variables, call the function `plot_model` with as argument the model of your choice and with the argument `plot=\"feature\"`.\n","\n","How do you interpret these values?"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"nbTranslate":{"displayLangs":["*"],"hotkey":"","langInMainMenu":true,"sourceLang":"fr","targetLang":"en","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":5}